#!/usr/bin/env python3
"""
Pre-exploitation phase module for APTES
"""

import re
import os
import logging
import tempfile
from datetime import datetime
from urllib.parse import urlparse

from phases.base import PhaseBase
from lib import validators, webanalysis, payloads, tools
from utils import reporting
from lib.tools import (
    check_owasp_top10_vulnerabilities,
    detect_modern_web_frameworks,
    detect_technology_stack,
    find_sensitive_info_in_js,
    analyze_csp,
    detect_dom_xss,
    find_hidden_parameters
)

# Check for optional imports
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

try:
    import concurrent.futures
    CONCURRENT_AVAILABLE = True
except ImportError:
    CONCURRENT_AVAILABLE = False

try:
    from openpyxl.styles import PatternFill
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False

class PreExploitationPhase(PhaseBase):
    """Pre-Exploitation Phase Controller"""
    
    def __init__(self, framework):
        """Initialize the Pre-Exploitation phase controller"""
        super().__init__(framework)
        
        # Get recon results from framework
        self.recon_results = framework.results.get("recon", {})
        
        # Initialize phase-specific results
        self.results.update({
            "vulnerability_validation": {},
            "webapp_analysis": {},
            "credential_testing": {},
            "payload_generation": {},
            "sqlmap_results": {},
            "xss_results": {},
            "cms_results": {},
            "lfi_results": {},
            "attack_vectors": [],
            "owasp_top10": {},
            "technology_stack": {},
            "frameworks": {},
            "sensitive_info": {},
            "csp_analysis": {},
            "dom_xss": {},
            "hidden_parameters": {}
        })
        
        # Risk colors for reports
        if EXCEL_AVAILABLE:
            self.risk_colors = {
                "critical": PatternFill(start_color="FF0000", end_color="FF0000", fill_type="solid"),
                "high": PatternFill(start_color="FFA500", end_color="FFA500", fill_type="solid"),
                "medium": PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid"),
                "low": PatternFill(start_color="00FF00", end_color="00FF00", fill_type="solid"),
                "info": PatternFill(start_color="ADD8E6", end_color="ADD8E6", fill_type="solid")
            }
    
    def _execute(self, exploit_filter=None, skip_web=False, skip_creds=False, skip_payloads=False, 
                 skip_sqlmap=False, skip_xss=False, skip_cms=False, skip_lfi=False):
        """
        Execute pre-exploitation phase operations
        
        Args:
            exploit_filter (dict): Filter for vulnerabilities and attacks
            skip_web (bool): Skip web application analysis
            skip_creds (bool): Skip credential testing
            skip_payloads (bool): Skip payload generation
            skip_sqlmap (bool): Skip SQLMap scanning
            skip_xss (bool): Skip XSS scanning
            skip_cms (bool): Skip CMS vulnerability scanning
            skip_lfi (bool): Skip LFI vulnerability scanning
        """
        self.logger.info(f"Starting pre-exploitation analysis for {self.target}")
        
        # Create a work directory for scan outputs
        self.work_dir = tempfile.mkdtemp(prefix="aptes_preexploit_")
        self.logger.info(f"Using work directory: {self.work_dir}")
        
        # Validate vulnerabilities
        self.validate_vulnerabilities()
        
        # Perform web application analysis if not skipped
        if not skip_web:
            self.analyze_web_applications()
            
            # Run SQLMap on potentially vulnerable URLs if not skipped
            if not skip_sqlmap:
                self.run_sql_injection_scans()
            
            # Run XSS scanning on potentially vulnerable URLs if not skipped
            if not skip_xss:
                self.run_xss_scans()
            
            # Run CMS vulnerability checks if not skipped
            if not skip_cms:
                self.check_cms_vulnerabilities()
            
            # Run LFI vulnerability checks if not skipped
            if not skip_lfi:
                self.run_lfi_checks()
        
        # Test default credentials if not skipped
        if not skip_creds:
            self.test_default_credentials()
        
        # Generate payloads if not skipped
        if not skip_payloads:
            self.generate_payloads()
        
        # Identify attack vectors
        self.identify_attack_vectors(exploit_filter)

        # OWASP Top 10 coverage
        owasp_results = check_owasp_top10_vulnerabilities(self.framework.target)
        self.results["owasp_top10"] = owasp_results

        # Technology stack detection
        html_content = self._get_html_content()
        headers = self._get_headers()
        stack_info = detect_technology_stack(html_content, headers)
        self.results["technology_stack"] = stack_info

        # Modern framework detection
        frameworks = detect_modern_web_frameworks(self.framework.target, html_content)
        self.results["frameworks"] = frameworks

        # Sensitive info in JS
        js_content = self._get_js_content()
        sensitive_info = find_sensitive_info_in_js(js_content)
        self.results["sensitive_info"] = sensitive_info

        # CSP analysis
        csp_analysis = analyze_csp(headers)
        self.results["csp_analysis"] = csp_analysis

        # DOM XSS detection
        dom_xss = detect_dom_xss(html_content, js_content)
        self.results["dom_xss"] = dom_xss

        # Hidden parameter discovery
        hidden_params = find_hidden_parameters(html_content)
        self.results["hidden_parameters"] = hidden_params
        
        self.logger.info(f"Pre-exploitation analysis completed for {self.target}")
        return self.results
    
    def validate_vulnerabilities(self):
        """
        Validate vulnerabilities from reconnaissance phase
        
        Returns:
            list: Validated vulnerabilities
        """
        self.logger.info("Validating vulnerabilities")
        
        validated_vulns = []
        
        # Extract vulnerabilities from recon results
        if self.recon_results and "active" in self.recon_results and "vulnerabilities" in self.recon_results["active"]:
            vulns = self.recon_results["active"]["vulnerabilities"].get("vulnerabilities", [])
            
            # Process each vulnerability
            for vuln in vulns:
                validated_vuln = validators.validate_vulnerability(self.target, vuln, verify_ssl=self.verify_ssl)
                if validated_vuln:
                    validated_vulns.append(validated_vuln)
        
        # Add manual vulnerability checks
        if self.recon_results and "active" in self.recon_results and "ports" in self.recon_results["active"]:
            for host, host_data in self.recon_results["active"]["ports"].items():
                for proto, proto_data in host_data.items():
                    for port, port_data in proto_data.items():
                        service = port_data.get("service", "").lower()
                        product = port_data.get("product", "").lower()
                        version = port_data.get("version", "")
                        
                        # Check for potential vulnerabilities
                        manual_vulns = validators.check_known_vulnerabilities(host, port, service, product, version)
                        validated_vulns.extend(manual_vulns)
        
        # Group vulnerabilities by host and service
        grouped_vulns = {}
        for vuln in validated_vulns:
            host = vuln.get("host", self.target)
            service = vuln.get("service", "unknown")
            
            if host not in grouped_vulns:
                grouped_vulns[host] = {}
            
            if service not in grouped_vulns[host]:
                grouped_vulns[host][service] = []
            
            grouped_vulns[host][service].append(vuln)
        
        self.results["vulnerability_validation"] = {
            "vulnerabilities": validated_vulns,
            "grouped": grouped_vulns,
            "total_count": len(validated_vulns)
        }
        
        self.logger.info(f"Validated {len(validated_vulns)} vulnerabilities")
        return validated_vulns
    
    def analyze_web_applications(self):
        """
        Analyze web applications for vulnerabilities
        
        Returns:
            dict: Analysis results
        """
        self.logger.info("Analyzing web applications")
        
        web_services = []
        
        # Extract web services from recon results
        if self.recon_results and "active" in self.recon_results and "ports" in self.recon_results["active"]:
            for host, host_data in self.recon_results["active"]["ports"].items():
                for proto, proto_data in host_data.items():
                    for port, port_data in proto_data.items():
                        service = port_data.get("service", "").lower()
                        if "http" in service or port in ["80", "443", "8080", "8443"]:
                            protocol = "https" if port in ["443", "8443"] or "https" in service else "http"
                            web_services.append({
                                "host": host,
                                "port": port,
                                "url": f"{protocol}://{host}:{port}"
                            })
        
        # If no web services found in recon, try common ports
        if not web_services:
            for port in [80, 443, 8080, 8443]:
                protocol = "https" if port in [443, 8443] else "http"
                web_services.append({
                    "host": self.target,
                    "port": str(port),
                    "url": f"{protocol}://{self.target}:{port}"
                })
        
        # Analyze each web service
        web_results = {
            "services": [],
            "findings": []
        }
        
        if CONCURRENT_AVAILABLE:
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.threads) as executor:
                future_to_service = {executor.submit(self._analyze_web_service, service): service for service in web_services}
                
                for future in concurrent.futures.as_completed(future_to_service):
                    service = future_to_service[future]
                    try:
                        result = future.result()
                        if result:
                            web_results["services"].append(result["service"])
                            web_results["findings"].extend(result["findings"])
                    except Exception as e:
                        self.logger.error(f"Error analyzing web service {service.get('url')}: {str(e)}")
        else:
            # Sequential analysis if concurrent not available
            for service in web_services:
                try:
                    result = self._analyze_web_service(service)
                    if result:
                        web_results["services"].append(result["service"])
                        web_results["findings"].extend(result["findings"])
                except Exception as e:
                    self.logger.error(f"Error analyzing web service {service.get('url')}: {str(e)}")
        
        # Group findings by category
        grouped_findings = {}
        for finding in web_results["findings"]:
            category = finding.get("category", "Other")
            if category not in grouped_findings:
                grouped_findings[category] = []
            grouped_findings[category].append(finding)
        
        web_results["grouped_findings"] = grouped_findings
        web_results["total_findings"] = len(web_results["findings"])
        
        self.results["webapp_analysis"] = web_results
        self.logger.info(f"Web application analysis complete: {len(web_results['findings'])} findings")
        return web_results
    
    def _analyze_web_service(self, service):
        """
        Analyze a single web service
        
        Args:
            service (dict): Web service information
        
        Returns:
            dict: Service analysis results
        """
        if not REQUESTS_AVAILABLE:
            self.logger.error("Requests library not available, skipping web analysis")
            return None
        
        url = service.get("url")
        host = service.get("host")
        port = service.get("port")
        
        self.logger.debug(f"Analyzing web application at {url}")
        
        # Use the web analyzer from lib
        return webanalysis.analyze_web_service(host, port, url, verify_ssl=self.verify_ssl)
    
    def _get_html_content(self):
        """
        Retrieve HTML content for the main web target.
        Returns:
            str: HTML content as string, or empty string if not available.
        """
        # Try to get the first HTTP/HTTPS service from recon results
        url = self._get_main_web_url()
        if not url:
            return ""
        try:
            import requests
            resp = requests.get(url, timeout=10, verify=False)
            if resp.status_code == 200:
                return resp.text
        except Exception as e:
            self.logger.debug(f"Failed to fetch HTML content from {url}: {e}")
        return ""

    def _get_headers(self):
        """
        Retrieve HTTP headers for the main web target.
        Returns:
            dict: Headers as dictionary, or empty dict if not available.
        """
        url = self._get_main_web_url()
        if not url:
            return {}
        try:
            import requests
            resp = requests.get(url, timeout=10, verify=False)
            return dict(resp.headers)
        except Exception as e:
            self.logger.debug(f"Failed to fetch headers from {url}: {e}")
        return {}

    def _get_js_content(self):
        """
        Retrieve JavaScript content for the main web target (first external JS file found).
        Returns:
            str: JavaScript content as string, or empty string if not available.
        """
        html = self._get_html_content()
        if not html:
            return ""
        import re
        import requests
        from urllib.parse import urljoin
        url = self._get_main_web_url()
        # Find first external JS file
        match = re.search(r'<script[^>]+src=["\']([^"\']+\.js)["\']', html, re.IGNORECASE)
        if match:
            js_url = match.group(1)
            if not js_url.startswith("http"):
                js_url = urljoin(url, js_url)
            try:
                resp = requests.get(js_url, timeout=10, verify=False)
                if resp.status_code == 200:
                    return resp.text
            except Exception as e:
                self.logger.debug(f"Failed to fetch JS content from {js_url}: {e}")
        return ""

    def _get_main_web_url(self):
        """
        Helper to get the main web URL from recon results.
        Returns:
            str: URL or None
        """
        # Try to get from recon results
        if self.recon_results and "active" in self.recon_results and "ports" in self.recon_results["active"]:
            for host, host_data in self.recon_results["active"]["ports"].items():
                for proto, proto_data in host_data.items():
                    for port, port_data in proto_data.items():
                        service = port_data.get("service", "").lower()
                        if "http" in service or port in ["80", "443", "8080", "8443"]:
                            protocol = "https" if port in ["443", "8443"] or "https" in service else "http"
                            return f"{protocol}://{host}:{port}"
        # Fallback to self.target
        return f"http://{self.target}:80"

    def run_sql_injection_scans(self):
        """
        Run SQL injection scans on potential targets
        
        Returns:
            dict: SQLMap scan results
        """
        self.logger.info("Running SQL injection scans")
        
        # Check if SQLMap is installed
        if not tools.check_sqlmap_installation():
            self.logger.warning("SQLMap not installed, skipping SQL injection scans")
            return {"error": "SQLMap not installed"}
        
        # Output directory for SQLMap results
        sqlmap_output_dir = os.path.join(self.work_dir, "sqlmap")
        os.makedirs(sqlmap_output_dir, exist_ok=True)
        
        potential_sql_targets = []
        
        # Extract potential targets from web findings
        if "webapp_analysis" in self.results and "findings" in self.results["webapp_analysis"]:
            for finding in self.results["webapp_analysis"]["findings"]:
                if "SQL" in finding.get("category", "") or "sql" in finding.get("finding", "").lower():
                    url = finding.get("url")
                    if url:
                        potential_sql_targets.append({
                            "url": url,
                            "finding": finding
                        })
        
        # If no specific SQL findings, check all URLs with parameters
        if not potential_sql_targets and "webapp_analysis" in self.results and "services" in self.results["webapp_analysis"]:
            for service in self.results["webapp_analysis"]["services"]:
                url = service.get("url", "")
                if "?" in url:
                    potential_sql_targets.append({
                        "url": url,
                        "finding": {"category": "Potential SQL Injection Point"}
                    })
        
        # Run SQLMap against each target
        sqlmap_results = {
            "scan_count": len(potential_sql_targets),
            "vulnerable_count": 0,
            "scans": []
        }
        
        for target in potential_sql_targets:
            url = target["url"]
            target_output_dir = os.path.join(sqlmap_output_dir, urlparse(url).netloc.replace(":", "_"))
            
            self.logger.info(f"Running SQLMap against {url}")
            
            # Run SQLMap
            result = tools.run_sqlmap(url, output_dir=target_output_dir)
            
            # Add metadata to the result
            result["url"] = url
            result["timestamp"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            sqlmap_results["scans"].append(result)
            
            # Count vulnerable targets
            if result.get("vulnerable", False):
                sqlmap_results["vulnerable_count"] += 1
                
                # Add SQLMap findings to web findings
                if "webapp_analysis" in self.results and "findings" in self.results["webapp_analysis"]:
                    for vuln in result.get("vulnerabilities", []):
                        self.results["webapp_analysis"]["findings"].append({
                            "host": urlparse(url).netloc.split(":")[0],
                            "port": urlparse(url).port or (443 if urlparse(url).scheme == "https" else 80),
                            "url": url,
                            "category": "SQL Injection",
                            "finding": f"SQLMap confirmed SQL injection in parameter '{vuln.get('parameter', 'unknown')}'",
                            "risk_level": "high",
                            "description": f"SQLMap detected {vuln.get('type', 'unknown')} SQL injection vulnerability in parameter '{vuln.get('parameter', 'unknown')}'.",
                            "recommendation": "Implement proper input validation and parameterized queries.",
                            "tool": "SQLMap",
                            "verified": True
                        })
        
        self.results["sqlmap_results"] = sqlmap_results
        self.logger.info(f"SQL injection scans complete: {sqlmap_results['vulnerable_count']} vulnerable targets found out of {sqlmap_results['scan_count']} scanned")
        return sqlmap_results
    
    def run_xss_scans(self):
        """
        Run Cross-Site Scripting (XSS) scans on potential targets
        
        Returns:
            dict: XSS scan results
        """
        self.logger.info("Running XSS scans")
        
        # Check if XSSniper is installed
        xssniper_available = tools.check_xssniper_installation()
        if not xssniper_available:
            self.logger.warning("XSSniper not installed, falling back to custom XSS scanner")
        
        potential_xss_targets = []
        
        # Extract potential targets from web findings
        if "webapp_analysis" in self.results and "findings" in self.results["webapp_analysis"]:
            for finding in self.results["webapp_analysis"]["findings"]:
                if "XSS" in finding.get("category", "") or "cross-site" in finding.get("finding", "").lower():
                    url = finding.get("url")
                    if url:
                        potential_xss_targets.append({
                            "url": url,
                            "finding": finding
                        })
        
        # If no specific XSS findings, check all URLs with parameters
        if not potential_xss_targets and "webapp_analysis" in self.results and "services" in self.results["webapp_analysis"]:
            for service in self.results["webapp_analysis"]["services"]:
                url = service.get("url", "")
                if "?" in url:
                    potential_xss_targets.append({
                        "url": url,
                        "finding": {"category": "Potential XSS Point"}
                    })
        
        # Run XSS scanner against each target
        xss_results = {
            "scan_count": len(potential_xss_targets),
            "vulnerable_count": 0,
            "scans": []
        }
        
        for target in potential_xss_targets:
            url = target["url"]
            
            self.logger.info(f"Running XSS scan against {url}")
            
            # Run XSSniper if available, otherwise use custom scanner
            if xssniper_available:
                result = tools.run_xssniper(url)
            else:
                result = tools.run_custom_xss_scan(url)
            
            # Add metadata to the result
            result["url"] = url
            result["timestamp"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            xss_results["scans"].append(result)
            
            # Count vulnerable targets
            if result.get("vulnerable", False):
                xss_results["vulnerable_count"] += 1
                
                # Add XSS findings to web findings
                if "webapp_analysis" in self.results and "findings" in self.results["webapp_analysis"]:
                    for vuln in result.get("vulnerabilities", []):
                        self.results["webapp_analysis"]["findings"].append({
                            "host": urlparse(url).netloc.split(":")[0],
                            "port": urlparse(url).port or (443 if urlparse(url).scheme == "https" else 80),
                            "url": url,
                            "category": "Cross-Site Scripting (XSS)",
                            "finding": f"XSS vulnerability confirmed in parameter '{vuln.get('parameter', 'unknown')}'",
                            "risk_level": "high",
                            "description": f"XSS vulnerability of type {vuln.get('type', 'reflected')} detected in parameter '{vuln.get('parameter', 'unknown')}'.",
                            "recommendation": "Implement proper output encoding and content security policy.",
                            "tool": "XSSniper" if xssniper_available else "Custom XSS Scanner",
                            "verified": True,
                            "payload": vuln.get("payload", "")
                        })
        
        self.results["xss_results"] = xss_results
        self.logger.info(f"XSS scans complete: {xss_results['vulnerable_count']} vulnerable targets found out of {xss_results['scan_count']} scanned")
        return xss_results
    
    def check_cms_vulnerabilities(self):
        """
        Check for CMS vulnerabilities in web applications
        
        Returns:
            dict: CMS vulnerability check results
        """
        self.logger.info("Checking for CMS vulnerabilities")
        
        cms_targets = []
        
        # Get all web service URLs
        if "webapp_analysis" in self.results and "services" in self.results["webapp_analysis"]:
            for service in self.results["webapp_analysis"]["services"]:
                url = service.get("url", "")
                if url:
                    cms_targets.append(url)
        
        # Run CMS vulnerability checks on each target
        cms_results = {
            "scan_count": len(cms_targets),
            "detected_count": 0,
            "vulnerable_count": 0,
            "scans": []
        }
        
        for url in cms_targets:
            self.logger.info(f"Checking for CMS vulnerabilities at {url}")
            
            # Run CMS vulnerability check
            result = tools.run_cms_vulnerability_check(url)
            
            # Add metadata to the result
            result["url"] = url
            result["timestamp"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            cms_results["scans"].append(result)
            
            # Count detected CMS
            if result.get("cms_detected", False):
                cms_results["detected_count"] += 1
                
                # Add to web findings
                if "webapp_analysis" in self.results and "findings" in self.results["webapp_analysis"]:
                    self.results["webapp_analysis"]["findings"].append({
                        "host": urlparse(url).netloc.split(":")[0],
                        "port": urlparse(url).port or (443 if urlparse(url).scheme == "https" else 80),
                        "url": url,
                        "category": "CMS Detection",
                        "finding": f"Detected {result.get('cms_name')} {result.get('cms_version', '')}",
                        "risk_level": "info",
                        "description": f"Identified {result.get('cms_name')} {result.get('cms_version', '')} CMS installation.",
                        "recommendation": "Keep CMS and plugins updated to latest versions."
                    })
                
                # Check for vulnerabilities
                if "vulnerabilities" in result and result["vulnerabilities"]:
                    cms_results["vulnerable_count"] += 1
                    
                    # Add each vulnerability to the web findings
                    for vuln in result["vulnerabilities"]:
                        if "webapp_analysis" in self.results and "findings" in self.results["webapp_analysis"]:
                            self.results["webapp_analysis"]["findings"].append({
                                "host": urlparse(url).netloc.split(":")[0],
                                "port": urlparse(url).port or (443 if urlparse(url).scheme == "https" else 80),
                                "url": vuln.get("url", url),
                                "category": "CMS Vulnerability",
                                "finding": vuln.get("name", "Unknown CMS vulnerability"),
                                "risk_level": vuln.get("risk_level", "medium"),
                                "description": vuln.get("description", "No description available"),
                                "recommendation": "Update CMS to the latest version and apply security patches.",
                                "cve": vuln.get("cve", "No CVE assigned")
                            })
        
        self.results["cms_results"] = cms_results
        self.logger.info(f"CMS vulnerability checks complete: {cms_results['detected_count']} CMS detected, {cms_results['vulnerable_count']} vulnerable")
        return cms_results
    
    def run_lfi_checks(self):
        """
        Run Local File Inclusion (LFI) vulnerability checks
        
        Returns:
            dict: LFI check results
        """
        self.logger.info("Running LFI vulnerability checks")
        
        potential_lfi_targets = []
        
        # Extract potential targets from web findings
        if "webapp_analysis" in self.results and "findings" in self.results["webapp_analysis"]:
            for finding in self.results["webapp_analysis"]["findings"]:
                if "LFI" in finding.get("category", "") or "file inclusion" in finding.get("finding", "").lower():
                    url = finding.get("url")
                    if url:
                        potential_lfi_targets.append({
                            "url": url,
                            "finding": finding
                        })
        
        # Add URLs with parameters that might be vulnerable to LFI
        if "webapp_analysis" in self.results and "services" in self.results["webapp_analysis"]:
            for service in self.results["webapp_analysis"]["services"]:
                url = service.get("url", "")
                parsed_url = urlparse(url)
                query = parsed_url.query
                
                # Check if URL has parameters that might be vulnerable to LFI
                if "?" in url and any(param in query.lower() for param in ["file", "page", "include", "path", "doc", "document", "folder", "root", "view"]):
                    potential_lfi_targets.append({
                        "url": url,
                        "finding": {"category": "Potential LFI Point"}
                    })
        
        # Run LFI checks against each target
        lfi_results = {
            "scan_count": len(potential_lfi_targets),
            "vulnerable_count": 0,
            "scans": []
        }
        
        for target in potential_lfi_targets:
            url = target["url"]
            
            self.logger.info(f"Running LFI check against {url}")
            
            # Run LFI tester
            result = tools.run_lfi_tester(url)
            
            # Add metadata to the result
            result["url"] = url
            result["timestamp"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            lfi_results["scans"].append(result)
            
            # Count vulnerable targets
            if result.get("vulnerable", False):
                lfi_results["vulnerable_count"] += 1
                
                # Add LFI findings to web findings
                if "webapp_analysis" in self.results and "findings" in self.results["webapp_analysis"]:
                    for vuln in result.get("vulnerabilities", []):
                        self.results["webapp_analysis"]["findings"].append({
                            "host": urlparse(url).netloc.split(":")[0],
                            "port": urlparse(url).port or (443 if urlparse(url).scheme == "https" else 80),
                            "url": url,
                            "category": "Local File Inclusion (LFI)",
                            "finding": f"LFI vulnerability confirmed in parameter '{vuln.get('parameter', 'unknown')}'",
                            "risk_level": "high",
                            "description": f"Local File Inclusion vulnerability detected in parameter '{vuln.get('parameter', 'unknown')}'. "
                                          f"Successfully accessed {vuln.get('indicator', 'sensitive file')} using payload: {vuln.get('payload', 'unknown')}",
                            "recommendation": "Validate and sanitize file paths, use whitelists of allowed files.",
                            "tool": "LFI Tester",
                            "verified": True,
                            "payload": vuln.get("payload", "")
                        })
        
        self.results["lfi_results"] = lfi_results
        self.logger.info(f"LFI checks complete: {lfi_results['vulnerable_count']} vulnerable targets found out of {lfi_results['scan_count']} scanned")
        return lfi_results
    
    def test_default_credentials(self):
        """
        Test for default credentials
        
        Returns:
            dict: Credential test results
        """
        self.logger.info("Testing for default credentials")
        
        cred_results = {
            "tested_services": [],
            "findings": []
        }
        
        # Extract services to test from recon results
        services_to_test = []
        if self.recon_results and "active" in self.recon_results and "ports" in self.recon_results["active"]:
            for host, host_data in self.recon_results["active"]["ports"].items():
                for proto, proto_data in host_data.items():
                    for port, port_data in proto_data.items():
                        service = port_data.get("service", "").lower()
                        
                        if service in ["http", "https", "ssh", "ftp", "telnet", "mysql", "mssql", "postgres", "snmp"]:
                            services_to_test.append({
                                "host": host,
                                "port": port,
                                "service": service,
                                "product": port_data.get("product", ""),
                                "version": port_data.get("version", "")
                            })
        
        # If no services found in recon, test common services
        if not services_to_test:
            for service_info in [
                {"service": "http", "port": "80"},
                {"service": "https", "port": "443"},
                {"service": "ssh", "port": "22"},
                {"service": "ftp", "port": "21"}
            ]:
                services_to_test.append({
                    "host": self.target,
                    "port": service_info["port"],
                    "service": service_info["service"]
                })
        
        # Test each service
        if CONCURRENT_AVAILABLE:
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.threads) as executor:
                future_to_service = {
                    executor.submit(
                        tools.run_default_credentials_check, 
                        service["host"], 
                        service["service"], 
                        service["port"]
                    ): service for service in services_to_test
                }
                
                for future in concurrent.futures.as_completed(future_to_service):
                    service = future_to_service[future]
                    try:
                        result = future.result()
                        if result:
                            cred_results["tested_services"].append({
                                "host": service["host"],
                                "port": service["port"],
                                "service": service["service"]
                            })
                            
                            # If successful logins found, add to findings
                            if result.get("successful_logins", []):
                                for cred in result["successful_logins"]:
                                    finding = {
                                        "host": service["host"],
                                        "port": service["port"],
                                        "service": service["service"],
                                        "finding": f"Default credentials found: {cred.get('username', '')}:{cred.get('password', '')}",
                                        "category": "Default Credentials",
                                        "risk_level": "high",
                                        "description": f"Default or weak credentials were found for {service['service']} service.",
                                        "credentials": f"{cred.get('username', '')}:{cred.get('password', '')}",
                                        "recommendation": "Change default credentials and implement strong password policies."
                                    }
                                    cred_results["findings"].append(finding)
                    except Exception as e:
                        self.logger.error(f"Error testing credentials for {service['service']} on {service['host']}:{service['port']}: {str(e)}")
        else:
            # Sequential testing if concurrent not available
            for service in services_to_test:
                try:
                    result = tools.run_default_credentials_check(
                        service["host"], 
                        service["service"], 
                        service["port"]
                    )
                    
                    if result:
                        cred_results["tested_services"].append({
                            "host": service["host"],
                            "port": service["port"],
                            "service": service["service"]
                        })
                        
                        # If successful logins found, add to findings
                        if result.get("successful_logins", []):
                            for cred in result["successful_logins"]:
                                finding = {
                                    "host": service["host"],
                                    "port": service["port"],
                                    "service": service["service"],
                                    "finding": f"Default credentials found: {cred.get('username', '')}:{cred.get('password', '')}",
                                    "category": "Default Credentials",
                                    "risk_level": "high",
                                    "description": f"Default or weak credentials were found for {service['service']} service.",
                                    "credentials": f"{cred.get('username', '')}:{cred.get('password', '')}",
                                    "recommendation": "Change default credentials and implement strong password policies."
                                }
                                cred_results["findings"].append(finding)
                except Exception as e:
                    self.logger.error(f"Error testing credentials for {service['service']} on {service['host']}:{service['port']}: {str(e)}")
        
        cred_results["total_tested"] = len(cred_results["tested_services"])
        cred_results["total_findings"] = len(cred_results["findings"])
        
        self.results["credential_testing"] = cred_results
        self.logger.info(f"Credential testing complete: {len(cred_results['findings'])} findings from {len(cred_results['tested_services'])} services")
        
        return cred_results
    
    def generate_payloads(self):
        """
        Generate payloads for identified vulnerabilities
        
        Returns:
            dict: Generated payloads
        """
        self.logger.info("Generating payloads for identified vulnerabilities")
        
        payload_results = {
            "payloads": [],
            "payload_types": set()
        }
        
        # Generate payloads for validated vulnerabilities
        if "vulnerability_validation" in self.results and "vulnerabilities" in self.results["vulnerability_validation"]:
            vulns = self.results["vulnerability_validation"]["vulnerabilities"]
            
            for vuln in vulns:
                if vuln.get("risk_level") in ["critical", "high"]:
                    vuln_payload = payloads.generate_payload_for_vulnerability(vuln)
                    if vuln_payload:
                        payload_results["payloads"].append(vuln_payload)
                        payload_results["payload_types"].add(vuln_payload["type"])
        
        # Generate payloads based on web findings
        if "webapp_analysis" in self.results and "findings" in self.results["webapp_analysis"]:
            for finding in self.results["webapp_analysis"]["findings"]:
                if finding.get("risk_level") in ["critical", "high", "medium"]:
                    category = finding.get("category", "")
                    
                    finding_payload = None
                    
                    # Generate different payload types based on category
                    if "SQL" in category:
                        finding_payload = payloads.generate_sql_injection_payload(finding)
                    elif "XSS" in category or "Cross-Site Scripting" in category:
                        finding_payload = payloads.generate_xss_payload(finding)
                    elif "File Inclusion" in category or "LFI" in category:
                        finding_payload = payloads.generate_lfi_payload(finding)
                    elif "RCE" in category or "Remote Code Execution" in category or "Command Injection" in category:
                        finding_payload = payloads.generate_rce_payload(finding)
                    elif "Default Credentials" in category or "Weak Credentials" in category:
                        finding_payload = payloads.generate_default_creds_payload(finding)
                    
                    if finding_payload:
                        payload_results["payloads"].append(finding_payload)
                        payload_results["payload_types"].add(finding_payload["type"])
        
        # Include SQLMap results in payloads if available
        if "sqlmap_results" in self.results and "scans" in self.results["sqlmap_results"]:
            for scan in self.results["sqlmap_results"]["scans"]:
                if scan.get("vulnerable", False):
                    url = scan.get("url", "")
                    
                    # Create SQLMap command as payload
                    sqlmap_payload = {
                        "vulnerability": "SQL Injection",
                        "target": url,
                        "service": "http",
                        "type": "sql_injection",
                        "command": scan.get("command", ""),
                        "notes": "Payload generated from successful SQLMap scan",
                        "verified": True
                    }
                    
                    payload_results["payloads"].append(sqlmap_payload)
                    payload_results["payload_types"].add("sql_injection")
        
        # Include XSS scan results in payloads if available
        if "xss_results" in self.results and "scans" in self.results["xss_results"]:
            for scan in self.results["xss_results"]["scans"]:
                if scan.get("vulnerable", False) and "vulnerabilities" in scan:
                    url = scan.get("url", "")
                    
                    for vuln in scan["vulnerabilities"]:
                        # Create XSS payload
                        xss_payload = {
                            "vulnerability": "Cross-Site Scripting (XSS)",
                            "target": url,
                            "service": "http",
                            "type": "xss",
                            "parameter": vuln.get("parameter", "unknown"),
                            "payload": vuln.get("payload", "<script>alert('XSS')</script>"),
                            "notes": "Payload generated from successful XSS scan",
                            "verified": True
                        }
                        
                        payload_results["payloads"].append(xss_payload)
                        payload_results["payload_types"].add("xss")
        
        # Include LFI scan results in payloads if available
        if "lfi_results" in self.results and "scans" in self.results["lfi_results"]:
            for scan in self.results["lfi_results"]["scans"]:
                if scan.get("vulnerable", False) and "vulnerabilities" in scan:
                    url = scan.get("url", "")
                    
                    for vuln in scan["vulnerabilities"]:
                        # Create LFI payload
                        lfi_payload = {
                            "vulnerability": "Local File Inclusion (LFI)",
                            "target": url,
                            "service": "http",
                            "type": "lfi",
                            "parameter": vuln.get("parameter", "unknown"),
                            "payload": vuln.get("payload", "../../../etc/passwd"),
                            "notes": "Payload generated from successful LFI scan",
                            "verified": True
                        }
                        
                        payload_results["payloads"].append(lfi_payload)
                        payload_results["payload_types"].add("lfi")
        
        # Set payload counts and convert set to list
        payload_results["total_payloads"] = len(payload_results["payloads"])
        payload_results["payload_types"] = list(payload_results["payload_types"])
        
        self.results["payload_generation"] = payload_results
        self.logger.info(f"Generated {len(payload_results['payloads'])} payloads of {len(payload_results['payload_types'])} different types")
        return payload_results
    
    def identify_attack_vectors(self, exploit_filter=None):
        """
        Identify potential attack vectors based on findings
        
        Args:
            exploit_filter (dict): Filter for vulnerabilities by risk level
        
        Returns:
            list: Identified attack vectors
        """
        self.logger.info("Identifying attack vectors")
        
        attack_vectors = []
        
        # Process validated vulnerabilities
        if "vulnerability_validation" in self.results and "vulnerabilities" in self.results["vulnerability_validation"]:
            vulns = self.results["vulnerability_validation"]["vulnerabilities"]
            
            for vuln in vulns:
                risk_level = vuln.get("risk_level")
                
                if exploit_filter and "risk_level" in exploit_filter:
                    if risk_level not in exploit_filter["risk_level"]:
                        continue
                
                attack_vectors.append({
                    "type": "vulnerability",
                    "name": vuln.get("vulnerability"),
                    "target": f"{vuln.get('host')}:{vuln.get('port')}",
                    "service": vuln.get("service"),
                    "risk_level": risk_level,
                    "description": vuln.get("details", "No details available"),
                    "payload_available": any(p["type"] == vuln.get("vulnerability", "").lower() for p in self.results.get("payload_generation", {}).get("payloads", []))
                })
        
        # Process web app findings
        if "webapp_analysis" in self.results and "findings" in self.results["webapp_analysis"]:
            findings = self.results["webapp_analysis"]["findings"]
            
            for finding in findings:
                risk_level = finding.get("risk_level")
                
                if exploit_filter and "risk_level" in exploit_filter:
                    if risk_level not in exploit_filter["risk_level"]:
                        continue
                
                attack_vectors.append({
                    "type": "web",
                    "name": finding.get("finding"),
                    "target": finding.get("url", f"{finding.get('host')}:{finding.get('port')}"),
                    "category": finding.get("category"),
                    "risk_level": risk_level,
                    "description": finding.get("description", "No description available"),
                    "verified": finding.get("verified", False),
                    "tool": finding.get("tool", "Web Analysis"),
                    "payload_available": any(p["type"] == finding.get("category", "").lower().replace(" ", "_") 
                                            for p in self.results.get("payload_generation", {}).get("payloads", []))
                })
        
        # Process credential findings
        if "credential_testing" in self.results and "findings" in self.results["credential_testing"]:
            findings = self.results["credential_testing"]["findings"]
            
            for finding in findings:
                if "credentials" in finding:
                    attack_vectors.append({
                        "type": "credentials",
                        "name": finding.get("finding"),
                        "target": f"{finding.get('host')}:{finding.get('port')}",
                        "service": finding.get("service"),
                        "risk_level": finding.get("risk_level"),
                        "description": finding.get("description", "No description available"),
                        "credentials": finding.get("credentials")
                    })
        
        # Process SQLMap results
        if "sqlmap_results" in self.results and "scans" in self.results["sqlmap_results"]:
            for scan in self.results["sqlmap_results"]["scans"]:
                if scan.get("vulnerable", False):
                    url = scan.get("url", "")
                    
                    risk_level = "high"  # SQLi is typically high risk
                    
                    if exploit_filter and "risk_level" in exploit_filter:
                        if risk_level not in exploit_filter["risk_level"]:
                            continue
                    
                    attack_vectors.append({
                        "type": "sqlmap",
                        "name": f"SQLMap Confirmed SQL Injection in {url}",
                        "target": url,
                        "category": "SQL Injection",
                        "risk_level": risk_level,
                        "description": f"SQLMap confirmed SQL injection vulnerability in {url}",
                        "verified": True,
                        "tool": "SQLMap",
                        "command": scan.get("command", ""),
                        "payload_available": True
                    })
        
        # Process XSS scan results
        if "xss_results" in self.results and "scans" in self.results["xss_results"]:
            for scan in self.results["xss_results"]["scans"]:
                if scan.get("vulnerable", False):
                    url = scan.get("url", "")
                    
                    risk_level = "high"  # XSS is typically high risk
                    
                    if exploit_filter and "risk_level" in exploit_filter:
                        if risk_level not in exploit_filter["risk_level"]:
                            continue
                    
                    attack_vectors.append({
                        "type": "xss",
                        "name": f"XSS Vulnerability in {url}",
                        "target": url,
                        "category": "Cross-Site Scripting (XSS)",
                        "risk_level": risk_level,
                        "description": f"Cross-Site Scripting vulnerability confirmed in {url}",
                        "verified": True,
                        "tool": scan.get("command", "").startswith("python3 xssniper.py") and "XSSniper" or "Custom XSS Scanner",
                        "payload_available": True
                    })

    # Process CMS vulnerability results
        if "cms_results" in self.results and "scans" in self.results["cms_results"]:
            for scan in self.results["cms_results"]["scans"]:
                if scan.get("cms_detected", False) and "vulnerabilities" in scan and scan["vulnerabilities"]:
                    url = scan.get("url", "")
                    cms_name = scan.get("cms_name", "Unknown CMS")
                    cms_version = scan.get("cms_version", "")
                    
                    for vuln in scan["vulnerabilities"]:
                        risk_level = vuln.get("risk_level", "medium")
                        
                        if exploit_filter and "risk_level" in exploit_filter:
                            if risk_level not in exploit_filter["risk_level"]:
                                continue
                        
                        attack_vectors.append({
                            "type": "cms",
                            "name": vuln.get("name", f"CMS Vulnerability in {cms_name}"),
                            "target": vuln.get("url", url),
                            "category": "CMS Vulnerability",
                            "risk_level": risk_level,
                            "description": vuln.get("description", f"Vulnerability in {cms_name} {cms_version}"),
                            "verified": False,
                            "tool": "CMS Vulnerability Scanner",
                            "cms": cms_name,
                            "cms_version": cms_version,
                            "cve": vuln.get("cve", "")
                        })
        
        # Process LFI vulnerability results
        if "lfi_results" in self.results and "scans" in self.results["lfi_results"]:
            for scan in self.results["lfi_results"]["scans"]:
                if scan.get("vulnerable", False):
                    url = scan.get("url", "")
                    
                    risk_level = "high"  # LFI is typically high risk
                    
                    if exploit_filter and "risk_level" in exploit_filter:
                        if risk_level not in exploit_filter["risk_level"]:
                            continue
                    
                    attack_vectors.append({
                        "type": "lfi",
                        "name": f"Local File Inclusion in {url}",
                        "target": url,
                        "category": "Local File Inclusion (LFI)",
                        "risk_level": risk_level,
                        "description": f"Local File Inclusion vulnerability confirmed in {url}",
                        "verified": True,
                        "tool": "LFI Tester",
                        "payload_available": True
                    })
        
        # Sort attack vectors by risk level
        risk_order = {"critical": 1, "high": 2, "medium": 3, "low": 4, "info": 5}
        attack_vectors.sort(key=lambda x: risk_order.get(x["risk_level"], 99))
        
        self.results["attack_vectors"] = attack_vectors
        self.logger.info(f"Identified {len(attack_vectors)} potential attack vectors")
        return attack_vectors
    
    def generate_report(self, format="all"):
        """
        Generate report in specified format
        
        Args:
            format (str): Report format (json, excel, all)
        
        Returns:
            dict: Generated report filenames
        """
        reporter = reporting.ReportGenerator(
            results=self.framework.results,
            target=self.target,
            output_dir=self.output_dir
        )
        report_files = reporter.generate_report(format=format, phase="preexploit")

        # HTML Report
        try:
            html_report = reporter.generate_phase_reports("preexploit")
            report_files["html"] = html_report.get("html")
            self.logger.info(f"HTML report saved to {html_report.get('html')}")
        except Exception as e:
            self.logger.error(f"Error generating HTML report: {str(e)}")
            report_files["html"] = None

        return report_files
